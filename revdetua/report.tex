\documentclass[mirror, portugues]{revdetua}
%
% Valid options are:
%   portugues --------- main language is Portuguese
%   final ------------- final version (default)
%   times ------------- use times (postscript) fonts for text
%   mirror ------------ prints a mirror image of the paper (with dvips)
%   visiblelabels ----- \SL, \SN, \SP, \EL, \EN, etc. defined
%   invisiblelabels --- \SL, \SN, \SP, \EL, \EN, etc. not defined (default)
%
% Note: the final version should use the times fonts
% Note: the really final version should also use the mirror option

\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath} 
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}
\floatname{algorithm}{Algoritmo}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{float}
\usepackage{booktabs}
%-------------------------------------
% compiling:
% Recipe: xelatex
% Recipe: pdflatex -> bibtex -> pdflatex -> pdflatex
% Recipe: xelatex
%
% notas:
% rever se algorimtos e imagens estão onde devem
%-------------------------------------
\begin{document}

\Header{02}{3}{Dezembro}{2024}{1}

\title{Maximum Weight Cut Problem}
% MUDAR TITULO ALLEZ ALLEZ ALGO COM RANDOMIZADO
\author{Hugo Veríssimo - 124348 - hugoverissimo@ua.pt}
\maketitle

\begin{abstract}
... abstrato em ingles
% por fazer abstrato
\end{abstract}

\begin{resumo}
Este relatório apresenta a implementação e comparação de dois métodos para resolver o problema \textit{Maximum Weight Cut}: uma pesquisa exaustiva e uma heurística gulosa. O problema \textit{Maximum Weight Cut} con ESTE É O ANTIGO FAZER NOVO
\end{resumo}

\section{Introdução}

O problema \textit{Maximum Weight Cut} é um problema de otimização, que tem como objetivo encontrar o corte mais pesado num grafo não direcionado $G(V,E)$, onde $|V| = n$ vértices e $|E| = m$ arestas. Este corte envolve dividir os vértices do grafo em dois subconjuntos disjuntos $S$ e $T$, sendo que o corte é a soma dos pesos das arestas que ligam os vértices de $S$ aos vértices de $T$: $|E(S, T)|$ \cite{AG14}.

No passado relatório foram analisados algoritmos determinísticos para resolver o problema \textit{Maximum Weight Cut}, nomeadamente a pesquisa exaustiva e a heurística gulosa. Neste relatório, serão analisados novos algoritmos com um certo grau de estocasticidade, com o objetivo de encontrar um algortimo que otimize o equilíbrio entre a complexidade computacional e a qualidade da solução obtida.

% POR ACABAR INTRO
para alem disso os resultados são comparados aos obtidos anteriormente

serao entao implexmentados 3 algoritmos, nomeadamente: ... e ...

DIZER ALGURES Q O NUMERO DE OPERACOES SERA A METRICA USADA PARA CONFIRMAR A COMPLEXIDADE DOS ALGORTIMO

\section{Metodologia da Análise}

Com o intuito de analisar o problema em destaque, implementar os algoritmos referidos e comparar os resultados obtidos, foi utilizada a linguagem de programação \textit{Python}, devido à vasta variedade de bibliotecas que contém, facilitando a implementação eficiente e simplificada dos algoritmos necessários.

% ACABAR ISTO, CORRIGIR COM OS PRINCIPAIS Q USAR REALMENTE
Sem desmerecer o uso de ficheiros auxiliares, a análise desenvolvida pode ser dividida em 2 ficheiros principais, sendo estes:
\begin{verbatim}
    $ python3 benchmarks.py
\end{verbatim}
% .......

Para a realização da análise dos algoritmos criados, foram utilizados grafos gerados aleatoriamente, com a semente $124348$, com diferentes números de vértices e densidade de arestas, e os grafos da coleção \textit{Gset}, disponibilizada por Yinyu Ye \cite{GS24}.

\section{Algoritmo de Corte Aleatório}

O primeiro algoritmo a ser implementado é um algoritmo de corte aleatório, que consiste em gerar várias soluções aleatórias e comparar as mesmas, escolhendo a melhor solução \cite{AG14}.

Este será um algortimo computacionalmente leve, pela sua simplicidade, mas não garante a obtenção da solução ótima, devido à sua natureza aleatória, sendo que a probabilidade de encontrar a mesma, assumindo que é única, é dada por

$$1 - \left( 1 - \frac{1}{2^{n-1}} \right)^{solutions}$$

\noindent onde $n$ é o número de vértices e $solutions$ é o número de soluções a gerar. Pode-se facilmente verificar que, para grafos de grandes dimensões, esta probabilidade decresce exponencialmente, tornando o algoritmo cada vez menos preciso.

Pelo facto do algoritmo gerar muitas soluções aleatórias, é importante garantir que não existem soluções repetidas a ser testadas, para evitar o cálculo do peso do corte, uma operação computacionalmente cara. Para isso é criado um \textit{set} onde serão guardadas as soluções já testadas, e cada vez que uma solução for gerada, a mesma só será testada depois de ser verificado que não é uma repetição. 

Atendendo à paragem do algoritmo, este tem dois critério de paragem, parando assim que um deles é verificado. O primeiro, e mais provável em grafos de grandes dimensões, é quando o número de soluções geradas atinge o limite, definido pelo utilizador. O segundo critério, é verificado quando todas as soluções possíveis foram testadas, ou seja, quando o \textit{set} que acompanha as soluções testadas contém $2^n$ elementos.

Este algoritmo pode ser então traduzido para o seguinte pseudocódigo:

\begin{algorithm}[H]
\raggedright
\textbf{Entrada:}

- lista de arestas e respetivos pesos (\textit{edges})

- número de vértices (\textit{n\_nodes})

- número de soluções a gerar (\textit{solutions})\\
\textbf{Saída:} subconjuntos \textit{S} e \textit{T}, peso do corte (\textit{weight}) \\
\hrule 
\caption{Corte Aleatório}
\begin{algorithmic}[1]
    \State \texttt{best\_solution} $\gets$ \texttt{None}
    \State \texttt{weight} $\gets$ 0
    \State \texttt{seen\_solutions} $\gets$ empty set
    \For{$i \gets 1$ \textbf{to} \texttt{solutions}}
        \State \texttt{partition} $\gets$ random partition of the nodes
        \If {\texttt{length}(\texttt{seen\_solutions}) $=$ $2^{\texttt{n\_nodes}}$}
            \State \textbf{break}
        \EndIf
        \State \texttt{partition\_hash} $\gets$ hash the partition
        \If {\texttt{partition\_hash} $\in$ \texttt{seen\_solutions}}
            \State \textbf{continue}
        \EndIf
        \State Add \texttt{partition\_hash} to \texttt{seen\_solutions}
        \State \texttt{new\_cut\_weight} $\gets$ compute the cut weight
        \If {\texttt{new\_cut\_weight} $>$ \texttt{weight}}
            \State \texttt{weight} $\gets$ \texttt{new\_cut\_weight}
            \State \texttt{best\_solution} $\gets$ copy of \texttt{partition}
        \EndIf
    \EndFor
    \State \texttt{S} $\gets$ set of nodes assigned to $0$ in \texttt{best\_solution}
    \State \texttt{T} $\gets$ set of nodes assigned to $1$ in \texttt{best\_solution}
    \Return \texttt{S}, \texttt{T}, \texttt{weight}
\end{algorithmic}
\end{algorithm}
    
Pode-se observar que a parte computacionalmente mais custosa deste algoritmo é o ciclo, que é responsável por gerar cortes aleatórios e calcular o peso dos mesmos. A complexidade deste ciclo é dado por $O(n + m)$, por percorrer todos os vértices atribuindo-as a um dos subconjuntos, e por calcular o peso do corte, percorrendo todas as arestas. Assim, a complexidade final do algoritmo é dada por $O((n + m) \times \texttt{solutions})$, visto que o ciclo corre no máximo \texttt{solutions} vezes. Esta complexidade pode ser simplifcada para $O(m)$, visto que $m \gg n$ para grafos densos e um $n$ grande, e que \texttt{solutions} é uma constante.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/ops_Random Sol.png}
    \caption{Número de operações básicas efetuadas pelo algoritmo de Corte Aleatório em função do número de arestas, para diferentes valores de \texttt{solutions} (MS).}
    \label{fig:random_ops}
\end{figure}

A complexidade referida é confirmada pela análise experimental apresentada na Fig. \ref{fig:random_ops}, onde se pode observar que o número de de operações básicas realizadas é linear em relação ao número de arestas do grafo, para diferentes números de soluções geradas.

% dizer q é m * solutions no grafico e q os pontos acima do reta tem a ver com qnd o m não é mt maior q n, mas que em grafos densos pronto

% dizer que o algortimo e mais rapido e isso qnd ms é 100 e coisas assim

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/sols_Random Sol.png}
    \caption{captio caption caption}
    \label{fig:sols_randomrandom}
\end{figure}


\section{Algoritmo de Simulated Annealing}

O segundo algoritmo a ser implementado é o algoritmo de \textit{Simulated Annealing} (SA), uma heurística de pesquisa aleatória, que consiste em encontrar soluções aproximadas para problemas de otimização combinatória \cite{SAT15}. Este método consite em gerar uma solução inicial aleatória, e a partir desta solução, gerar soluções vizinhas, que são soluções obtidas a partir da solução atual, e comparar as soluções, aceitando as soluções melhores e algumas piores, com uma probabilidade que decresce ao longo das iterações, até que a temperatura (parâmetro do algoritmo), que vai arrefecendo ao longo das iterações a uma determinada taxa de arrefecimento (paraâmetro do algoritmo), seja menor que um determinado valor, por exemplo $10^{-3}$, sendo este o critério de paragem do algoritmo \cite{SA87}.

Assim, é possível verificar que este algortimo tem como componente aleatória a seleção de uma solução inicial e a aceitação de soluções piores, e como parte determinística a diminuição da probabilidade de aceitação de soluções piores ao longo das iterações e a garantia de aceitação de soluções melhores.

Na Fig. \ref{fig:sa_aceitacao}, pode-se observar o comportamento decrescente da probabilidade de aceitação de soluções piores ao longo das iterações do algoritmo SA. Os pontos cinzentos representam números aleatórios gerados dentro do intervalo $[0,1]$ a cada iteração. A proposta de uma nova solução é aceite se esta for melhor que a solução atual ou se o valor aleatório (ponto cinzento) estiver abaixo da "curva" definida pelos pontos azuis. Esta curva reflete a probabilidade de aceitação, que diminui à medida que a temperatura decresce, limitando cada vez mais a aceitação de soluções subótimas.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/SA_DecrAceit.png}
    \caption{Probabilidade de aceitação de uma solução subótima, em função do número de iterações, quando o algoritmo de SA é aplicado com a semente $124348$, ao grafo G59.}
    \label{fig:sa_aceitacao}
\end{figure}

É também importante referir que o algoritmo não garante a não repetição de soluções previamente testadas, e não foi implementado um mecanismo para tal, visto que a probabilidade de testar a mesma solução é baixa para grafos de grandes dimensões, e o processo de comparação de soluções já testadas poderia prejudicar a eficiência do algoritmo.

% probabilidade de testar duas vezes a mesma solucao ?
% em 1 passo, n/n * 1/n
% 4 passos, n/n * 1/n * n-1/n * 1/n
% ....

O algoritmo de \textit{Simulated Annealing} pode ser examinado em detalhe no seguinte pseudocódigo:

\begin{algorithm}[H]
\raggedright
\textbf{Entrada:}

- lista de arestas e respetivos pesos (\textit{edges})

- temperatura (\textit{Temp})

- taxa de arrefecimento (\textit{cooling\_rate})\\
\textbf{Saída:} subconjuntos \textit{S} e \textit{T}, peso do corte (\textit{best\_cut}) \\
\hrule 
\caption{\textit{Simulated Annealing}}
\begin{algorithmic}[1]
    \State \texttt{partition} $\gets$ random partition of the nodes
    \State \texttt{best\_partition} $\gets$ \texttt{partition}
    \State \texttt{current\_cut} $\gets$ compute the cut weight
    \State \texttt{best\_cut} $\gets$ \texttt{current\_cut}
    \While{\texttt{Temp} \ensuremath{>} \ensuremath{10^{-3}}}
        \State \texttt{node} $\gets$ randomly select a node
        \State Flip the partition of \texttt{node} in \texttt{partition}
        \State \texttt{new\_cut} $\gets$ compute the new cut weight
        \State \texttt{cost\_diff} $\gets$ \texttt{new\_cut} $-$ \texttt{current\_cut}
        \If{\texttt{cost\_diff} \ensuremath{>} 0 \textbf{or} random number $\in$ [0, 1] \ensuremath{<} $e^{\ensuremath{\texttt{cost\_diff} / \texttt{Temp}}}}$
            \Comment Accept the move
            \State \texttt{current\_cut} $\gets$ \texttt{new\_cut}
            \If{\texttt{new\_cut} \ensuremath{>} \texttt{best\_cut}}
                \State \texttt{best\_cut} $\gets$ \texttt{new\_cut}
                \State \texttt{best\_partition} $\gets$ \texttt{partition}
            \EndIf
        \Else
            \Comment Reject the move
            \State Revert the partition of \texttt{node} in \texttt{partition}
        \EndIf
        \State \texttt{Temp} $\gets$ \texttt{Temp} \ensuremath{\times} \texttt{cooling\_rate}
    \EndWhile
    \State \texttt{S} $\gets$ set of nodes assigned to $0$ in \texttt{best\_partition}
    \State \texttt{T} $\gets$ set of nodes assigned to $1$ in \texttt{best\_partition} \\
    \Return \texttt{S}, \texttt{T}, \texttt{best\_cut}
\end{algorithmic}
\end{algorithm}

Tal como foi referido na descrição do algoritmo, pode-se verificar que este é sensível à solução inicial, pelo que será interessante executar o algoritmo várias vezes, cobrindo uma maior área do espaço de soluções.

Para além disso, através do pseudocódigo, é possível analisar a complexidade do algoritmo em questão. As operações computacionalmente mais custosas encontram-se dentro do ciclo, que só termina após a temperatura ser inferior a $10^{-3}$. Assim, torna-se importante calcular o total de iterações ($k$) que o ciclo irá realizar, o que pode ser feito através da seguinte equação:

\begin{align*}
    &\texttt{Temp}_0 \cdot (\texttt{cooling\_rate})^k \le 10^{-3} \\
    \Leftrightarrow\ &  k \geq  \frac{\log\left(\frac{10^{-3}}{\texttt{Temp}_0}\right)}{\log(\texttt{cooling\_rate})} \\
    \Leftrightarrow\ & k = \left\lceil \frac{\log\left(\frac{10^{-3}}{\texttt{Temp}_0}\right)}{\log(\texttt{cooling\_rate})} \right\rceil
\end{align*}

\noindent Quanto à complexidade dentro do ciclo, a mesma é dada por $O(m)$, visto que a operação mais custosa é o cálculo do peso do corte, que percorre todas as arestas do grafo. Assim, a complexidade final do algoritmo é dada por $O(m \times k)$, e pelo facto de $k$ depender apenas da temperatura inicial e da taxa de arrefecimento, a complexidade final pode ser aproximada por $O(m)$.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/ops_Simulated .png}
    \caption{Número de operações básicas efetuadas pelo algoritmo de SA em função do número de arestas, para diferentes valores de \texttt{Temp} (T) e de \texttt{cooling\_rate} (CR).}
    \label{fig:sa_ops}
\end{figure}


Através da Fig. \ref{fig:sa_ops} que representa o número de operações básicas realizadas em função do número de arestas do grafo, para diferentes temperaturas iniciais e taxas de arrefecimento, pode-se verificar que a complexidade do algoritmo é linear em relação ao número de arestas do grafo, para diferentes temperaturas iniciais e taxas de arrefecimento.

% falar a dizer q t maior implica... e cr maior implica ...

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/sols_Simulated .png}
    \caption{captio caption caption}
    \label{fig:sols_satested}
\end{figure}


\section{Algoritmo Guloso Aleatório}

Por fim, o terceiro algoritmo a ser implementado é um algoritmo guloso aleatório, que segue uma heurística baseada numa abordagem gulosa, não garantido encontrar a solução ótima. Este algoritmo itera sobre todos os vértices do grafo e, para cada vértice, troca a sua partição, verificando se a nova configuração melhora a solução atual. Caso o peso do corte com o vértice na partição oposta seja maior que o atual, a solução é atualizada. O processo continua até que uma iteração completa seja realizada sem encontrar melhorias, momento em que o algoritmo termina.

Devido ao critério de paragem do algoritmo, este pode correr indefinidamente, devido à natureza aleatória da solução inicial, que pode estar a um grande número de iterações da solução estável que o algoritmo procura. Por isso, é adicionado um fator de ajuste do máximo de iterações (\textit{itLim}) ao algoritmo, tornando o número máximo de iterações do mesmo $m \times itLim$.

É também importante referir que, pelo factor na única componente aleatória deste algoritmo ser a criação de uma solução inicial, e como todas as iterações realizam alterações em direção à melhor solução, o algoritmo nunca irá testar a mesma solução mais que uma vez, pelo que manter um registo sobre as soluções já testadas não é necessário.

Este algoritmo pode ser representado em pseudocódigo da seguinte forma:

\begin{algorithm}[H]
\raggedright
\textbf{Entrada:}

- lista de arestas e respetivos pesos (\textit{edges})

- número de vértices (\textit{n\_nodes})

- fator de ajuste do máximo de iterações (\textit{itLim})\\
\textbf{Saída:} subconjuntos \textit{S} e \textit{T}, peso do corte (\textit{weight}) \\
\hrule 
\caption{Guloso Aleatório}
\begin{algorithmic}[1]
    \State \texttt{partition} $\gets$ random partition of the nodes
    \State \texttt{cut\_weight} $\gets$ compute the cut weight
    \State \texttt{improved} $\gets$ \texttt{True}
    \State \texttt{it\_limit} $\gets$ \texttt{len(edges)} \ensuremath{\times} \texttt{itLim}

    \While{\texttt{improved} \textbf{and} \texttt{it\_limit} \ensuremath{>} 0}
        \State \texttt{it\_limit} $\gets$ \texttt{it\_limit} $ - 1$
        \State \texttt{improved} $\gets$ \texttt{False}
        \For{\texttt{node} \textbf{in} \texttt{range}(\texttt{n\_nodes})}
            \State Flip the partition of \texttt{node} in \texttt{partition}
            \State \texttt{new\_cut\_weight} $\gets$ compute the cut weight
            \If{\texttt{new\_cut\_weight} \ensuremath{>} \texttt{cut\_weight}}
                \State \texttt{cut\_weight} $\gets$ \texttt{new\_cut\_weight}
                \State \texttt{improved} $\gets$ \texttt{True}
                \State \textbf{break}  \Comment{Stop iteration for this node}
            \EndIf
            \State Revert the partition of \texttt{node} in \texttt{partition}
        \EndFor
    \EndWhile

    \State \texttt{S} $\gets$ Set of nodes assigned to $0$ in \texttt{partition}
    \State \texttt{T} $\gets$ Set of nodes assigned to $1$ in \texttt{partition}
    \Return \texttt{S}, \texttt{T}, \texttt{cut\_weight}
\end{algorithmic}
\end{algorithm}

Através da análise do pseudocódigo, é possível verificar que o algoritmo guloso aleatório, tal como o algoritmo de \textit{Simulated Annealing}, é sensível à solução inicial, pelo que é importante executar o algoritmo várias vezes, para que seja coberta uma maior área do espaço de soluções, garantido uma maior probabilidade de encontrar uma solução próxima da ótima, ou até a própria.

Para além disso, ainda através da análise do pseudocódigo, é possível analisar a complexidade do algoritmo. Pelo facto das operações mais custosas serem o percorrer todos os vértices ($O(n)$) e calcular o novo peso do corte, ao mudar o vértice de partição ($O(m)$), e ambas se encontrarem dentro do ciclo principal, que itera no máximo $m \times \texttt{itLim}$ vezes, a complexidade final do algoritmo é dada por $O(m \times \texttt{itLim} \times n \times m)$. Esta complexidade pode ser reescrita e simplificada de diferentes maneiras, entre elas $O(m^2 \times n)$, visto que \texttt{itLim} é uma constante.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/ops_Random Greedy.png}
    \caption{Número de operações básicas efetuadas pelo algoritmo Guloso Aleatório em função do número de arestas, para diferentes valores de \texttt{itLim} (IT).}
    \label{fig:random_greedy}
\end{figure}

Através da Fig. \ref{fig:random_greedy}, que representa o número de operações básicas efutadas pelo algoritmo em função do número de arestas, para diferentes valores de \texttt{itLim} (IT), é possível validar a complexidade do algoritmo calculada analiticamente. Pode-se observar que as curvas apresentadas, dadas pela função $$f(m, \text{IT}) = m^{2.5} \times \text{IT}$$ conseguem descrever o comportamento do algoritmo para grafos de diferentes dimensões, confirmando a complexidade calculada. É importante referir que a complexidade calculada analiticamente foi simplificada para $O(m^{2.5} \times \texttt{itLim})$, visto que para grafos com maior número de vértices e de arestas, onde é mais interessante aplicar este tipo de algoritmos, tem-se que

$$m \approx \frac{n(n-1)}{2} \Rightarrow n \approx \sqrt{m}$$

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/sols_Random Greedy.png}
    \caption{captio caption caption}
    \label{fig:sols_randomgreedy}
\end{figure}




\section{Análise dos Resultados}

Definido e analisada a complexidade de cada algoritmo, torna-se importante analisar as restantes métricas relevantes, tais como o tempo de execução, o número de operações básicas realizadas, o número de soluções testadas, a precisão das soluções obtidas.
% reordenar com as subsecçoes, e ops basicas ja foram feitas antes
% dizer q tbm se vai ver heur e exaustivo do report 01 ?

Para esta análise, foram utlizados grafos gerados aleatoriamente com a semente $124348$ e os grafos do conjunto \textit{Gset}. Entre os diferentes grafos, podem ser encontrados diversos números de vértices e densidades de arestas, garantindo a representação de uma ampla gama de configurações estruturais e cenários computacionais.

Dada a componente estocástica dos algoritmos, para assegurar resultados mais precisos e consistentes, cada grafo foi processado múltiplas vezes, calculando-se a média para as diferentes métricas (número de operações básicas, precisão, etc.), com exceção do tempo de execução, para o qual foi calculado a mediana. A escolha da mediana para esta métrica deve-se à sua robustez contra valores atípicos que podem surgir devido à falta de controlo sobre o ambiente de execução dos algoritmos.


\subsection{Análise do Número de Operações}

Atendendo ao número de operações básicas que um algoritmo executa, esta métrica ao estar diretamente relacionado com a complexidade de um algoritmo, já foi analisada anteriormente, na secção correspondente a cada algoritmo, de modo a validar a complexidade calculada formalmente.

Na tabela \ref{table:numops}, pode-se observar a complexidade dos algoritmos em função do número de operações básicas, permitindo a comparação da eficiência computacional entre os diferentes algoritmos implementados.

\begin{table}[H]
\centering
\caption{Complexidade dos algoritmos pelo número de operações básicas.}
\label{table:numops}
\begin{tabular}{ll}
\toprule
\textbf{Algoritmo} & \textbf{Complexidade} \\
\midrule
Pesquisa Exaustiva & $O(2^n)$ \\
Pesquisa Gulosa & $O(m \log m)$ \\
Corte Aleatório & $O(m)$ \\
Simulated Annealing & $O(m)$ \\
Guloso Aleatório & $O(m^2 \times n)$ \\
\bottomrule
\end{tabular}
\end{table}

Tendo em conta que para grafos de maiores dimensões, com elevado número de arestas, a relação entre $n$ e $m$ é dada por $n^2 \approx m$, torna-se possível realizar a seguinte visualização

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/basicops.png}
    \caption{PLOT COMPLEXIDADES EM FUNCAO DE N}
    \label{fig:total_ops_per_alg}
\end{figure}



ns q ve se q ns qm sobe mais e isso


\subsection{2 the execution time }

- Determine the largest graph that you can process on your computer, without taking too much time.

- Estimate the execution time that would be required by much larger problem instances.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/time_all.png}
    \caption{captio caption caption}
    \label{fig:time_all}
\end{figure}

ns q n se ve bem, os grafos grandes ele e menos preciso e por causa do ambiente do pc e ns q fica assim meio coiso, n tem tantos para fazer mediana

ent corta-se e faz se so num intervalo para ter uma melhor percecao e ambiente mais controlado

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../assets/time_interval.png}
    \caption{captio caption caption}
    \label{fig:time_interval}
\end{figure}



\subsection{precision}

sdhuiashdiauhd


\begin{table}[H]
\centering
\caption{caption caption caption caption}
\label{table:precision}
\begin{tabular}{lrr}
\toprule
\textbf{Algoritmo} & \textbf{GPeq. Prec.} & \textbf{GSet Prec.} \\
\midrule
Pesquisa Exaustiva & $100\% \ (0)$ & $-$ \\
Pesquisa Gulosa & $93\% \ (6.9)$ & $61\% \ (23.1)$ \\
Corte Aleatório (MS: 100) & $92\% \ (6.3)$ & $41\% \ (35.6)$ \\
Corte Aleatório (MS: 1000) & $96\% \ (4.5)$ & $42\% \ (35.0)$ \\
Corte Aleatório (MS: 10000) & $98\% \ (2.4)$ & $43\% \ (34.6)$ \\
SA (T: 100, CR: 0.99) & $99\% \ (1.3)$ & $58\% \ (28.7)$ \\
SA (T: 1000, CR: 0.9) & $97\% \ (3.1)$ & $40\% \ (36.1)$ \\
SA (T: 1000, CR: 0.95) & $98\% \ (2.6)$ & $44\% \ (34.4)$ \\
SA (T: 1000, CR: 0.99) & $99\% \ (1.3)$ & $59\% \ (28.5)$ \\
Guloso Aleatório (IT: 0.01) & $75\% \ (14.5)$ & $58\% \ (30.1)$ \\
Guloso Aleatório (IT: 0.1) & $84\% \ (12.4)$ & $90\% \ (7.7)$ \\
Guloso Aleatório (IT: 2) & $96\% \ (3.7)$ & $88\% \ (8.4)$ \\
\bottomrule
\end{tabular}
\end{table}


ns q precisao e desvio padrao entre parentes

olha q alguns nao correram na totalidade no gset, meter * e dizer ns q nao correu em todos por ser demorado e ns q, mas de qq forma da uma ideia

\section{Conclusão}

dhaushdisu



\bibliography{refs}

\end{document}
